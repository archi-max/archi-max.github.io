<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="/blog/tailwind-edf29ab1.css" />
    <link rel="icon" type="image/svg+xml" href="/blog/favicon.svg" />
    <script>window.__SCRATCH_BASE__ = "/blog"; window.__SCRATCH_SSG__ = true;</script>
      <title>[Application] FIG Fellowship — Insurance &amp; Liability for Frontier AI Safety</title>
    <meta name="description" content="My FIG Fellowship application and assessment submission on insurance/liability mechanisms for frontier AI safety.">
    <meta name="author" content="Ansh Tulsyan">
    <meta property="og:title" content="[Application] FIG Fellowship — Insurance &amp; Liability for Frontier AI Safety">
    <meta property="og:description" content="My FIG Fellowship application and assessment submission on insurance/liability mechanisms for frontier AI safety.">
    <meta property="og:type" content="article">
    <meta name="twitter:title" content="[Application] FIG Fellowship — Insurance &amp; Liability for Frontier AI Safety">
    <meta name="twitter:description" content="My FIG Fellowship application and assessment submission on insurance/liability mechanisms for frontier AI safety.">
    <meta name="twitter:card" content="summary_large_image">
    <meta property="article:tag" content="application">
    <meta property="article:tag" content="fig">
    <meta property="article:tag" content="ai governance">
    <meta property="article:tag" content="ai safety">
    <script>window.__scratch_author__ = "Ansh Tulsyan";</script>
  </head>
  <body>
    <div id="mdx"><link rel="preload" as="image" href="/blog/scratch-logo.svg"/><div class="min-h-screen bg-background text-body flex flex-col"><header class="fixed top-0 left-0 right-0 z-50 transition-all duration-300 bg-transparent translate-y-0"><div class="mx-auto max-w-[1180px] px-6 py-4"><nav class="flex items-center justify-between text-sm"><a href="/" class="inline-flex items-center gap-1.5 text-subtle hover:text-heading transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-left"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg>ansht.dev</a><div class="flex items-center gap-6"><a href="/" class="text-subtle hover:text-heading transition-colors">posts</a><button class="text-subtle hover:text-heading transition-colors p-1" aria-label="Toggle theme"><svg xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-moon"><path d="M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z"></path></svg></button></div></nav></div></header><main class="flex-1"><div class="mx-auto max-w-[1180px] px-6 pt-28 pb-16 lg:grid lg:grid-cols-[220px_minmax(0,1fr)] lg:gap-10 lg:items-start flex flex-col gap-10 overflow-visible"><div class="hidden lg:block -ml-6 lg:sticky lg:top-5"><aside class="w-[210px] lw-toc pr-4"><div class="relative pl-6 text-subtle"><div class="absolute left-1 top-0 bottom-0 w-px bg-border"></div><div class="flex items-center gap-2 mb-3 text-[0.68rem] uppercase tracking-[0.18em] text-heading"><span>Contents</span></div></div></aside></div><article class="lw-content max-w-3xl flex-1 prose prose-lg prose-headings:text-heading prose-headings:font-semibold prose-a:text-primary prose-a:underline prose-a:underline-offset-4 prose-a:transition-colors prose-blockquote:border-border prose-blockquote:not-italic"><h1>[Application] FIG Fellowship — Insurance &amp; Liability for Frontier AI Safety</h1><p>by <strong>Ansh Tulsyan</strong></p><br/><p>This post captures my FIG Fellowship application experience and what I submitted.</p><p>I applied to the FIG Fellowship project on <strong>Insurance and Liability as Levers for AI Safety</strong> and later completed the follow-on assessment task.</p><h2 id="what-i-submitted" class="group relative"><a href="#what-i-submitted" class="heading-anchor" aria-label="Link to What I submitted"></a>What I submitted</h2><h3 id="initial-application" class="group relative"><a href="#initial-application" class="heading-anchor" aria-label="Link to Initial application"></a>Initial application</h3><h3 id="why-would-you-like-to-join-the-fig-fellowship" class="group relative"><a href="#why-would-you-like-to-join-the-fig-fellowship" class="heading-anchor" aria-label="Link to Why would you like to join the FIG Fellowship?"></a>Why would you like to join the FIG Fellowship?</h3><p>At AMD, IP leakage is a major risk. That pushed me to think about:</p><ul>
<li>what a response policy should look like</li>
<li>what actions are realistic if leakage happens</li>
<li>who owns detection, triage, escalation, and containment</li>
</ul><p>A lot of teams optimize for typical LLM behavior, not rare high-impact edge cases. Tool-using agents increase blast radius: they can run commands, access internal knowledge, and accidentally expose privileged information. Insurance and liability mechanisms can create a real cost for ignoring these risks, and stronger incentives for better evals and permissioning.</p><h3 id="what-skills-and-experiences-could-you-contribute-to-the-projects-you-would-undertake-during-the-fig-fellowship" class="group relative"><a href="#what-skills-and-experiences-could-you-contribute-to-the-projects-you-would-undertake-during-the-fig-fellowship" class="heading-anchor" aria-label="Link to What skills and experiences could you contribute to the projects you would undertake during the FIG Fellowship?"></a>What skills and experiences could you contribute to the projects you would undertake during the FIG Fellowship?</h3><ul>
<li>Published an internal peer-reviewed benchmark for LLM performance in niche domains (including RCA and hardware-code error localization)</li>
<li>Deployed telemetry for AI workflows used by 1k+ engineers at AMD (logging, monitoring, and production-signal interpretation)</li>
<li>Translated safety practices like canary strings into tools for auditing IP leakage to LLMs</li>
<li>Fast empirical iteration with experiments and measurement-driven decisions</li>
</ul><h3 id="follow-on-assignment" class="group relative"><a href="#follow-on-assignment" class="heading-anchor" aria-label="Link to Follow-on assignment"></a>Follow-on assignment</h3><h4>Prompt</h4><blockquote>
<p>Identify an industry where insurance or liability frameworks significantly improved safety outcomes. What mechanisms drove this improvement, and what would need to be true for similar mechanisms to work for frontier AI systems?</p>
</blockquote><h4>Response</h4><p>Nuclear power offers a rare, double-edged lesson for frontier AI. It shows that insurance and liability frameworks can improve safety in catastrophic-risk industries by turning safety into an operational and financial constraint, not just a principle. It also shows the failure mode: if liability and compliance become slow, unpredictable, and lawyer-driven, the resulting friction can stall an industry. The goal for AI governance is to capture insurance’s safety discipline without recreating a regime where progress is choked by uncertainty and compliance grind.</p><p>A strong example is U.S. commercial nuclear power under the Price-Anderson framework. Operators must carry insurance and participate in an industry-wide retrospective pool that mutualizes liability if a major incident occurs. Crucially, the safety impact is not only “paying damages after the fact.” Coverage is conditioned on ongoing compliance, and insurers/mutuals developed loss-control capabilities: engineering inspections, peer review, underwriting standards, and the ability to require corrective action as a condition of coverage. That structure makes safety legible to senior leadership because safety performance affects premiums, operating terms, and, in the limit, the ability to keep operating.</p><p>Several mechanisms drove the improvement. First, mandatory insurance broadens participation: safety incentives apply across the whole operator set rather than only to the most conscientious. Second, underwriting and experience-based pricing translate performance into dollars, pushing investment toward prevention, monitoring, training, and robust operational processes. Third, insurer inspections tighten feedback loops because remediation demands can be immediate and commercially consequential. Fourth, mutualized exposure creates peer pressure: when one actor’s failure imposes costs on the entire pool, the group has reason to standardize best practices and scrutinize weak performers.</p><p>A direct transplant to AI will fail unless we account for its differences. Nuclear plants are scarce, immobile, and license-gated; frontier AI is digital, fast-iterating, and can be copied, fine-tuned, or open-sourced. Nuclear harms are physically measurable and attributable; many AI harms are multi-actor and multi-causal, and some are diffuse or gradual. Finally, AI risk can be highly correlated: the same base model deployed everywhere can create synchronized failure modes, which strains traditional insurability.</p><p>For similar mechanisms to work for frontier AI systems, several things would need to be true. Obligations must be risk-tiered and capability-triggered, not written for “today’s models”: heavyweight requirements should apply to frontier, high-impact deployments (capability, autonomy/tool access, scale, and domain criticality), while low-risk innovation stays cheap and fast. Static certification must become continuous benchmarking and “versioned safety cases,” with clear triggers for re-evaluation when models or deployment contexts change. Attribution and incident-data infrastructure must exist (logging, provenance where feasible, and standardized incident and near-miss reporting), so insurers can price and enforce based on evidence rather than guesswork.</p><p>Enforcement also needs layered chokepoints. Compute is a natural gate for public, frontier-scale capability because it requires substantial aggregate compute under common control, but it cannot be the only gate if actors can distribute work across smaller sites. A workable regime would combine compute and cloud/API deployment controls with power, colocation, and hardware traceability where feasible, making evasion through “many small datacenters” costly and risky. A regulated registry for frontier models and deployments can tie this together by assigning persistent identifiers and linking evaluation status, proof of coverage, and incident reporting to real deployments.</p><p>Finally, the system must be predictable and pro-competition. Clear standards and safe harbors should reduce open-ended legal uncertainty, and insurance should complement regulation rather than become an arbitrary private veto. If designed well, the nuclear incentive logic can carry over: pricing risk, demanding remediation, and creating peer accountability. The goal is a regime that updates as fast as frontier AI: periodic refresh cycles that strengthen with capability without freezing the field in place.</p><h2 id="reflection" class="group relative"><a href="#reflection" class="heading-anchor" aria-label="Link to Reflection"></a>Reflection</h2><p>Cool experience overall, and it got me thinking more seriously about insurance as an AI safety lever and what companies like <a href="https://aiuc.com" target="_blank" rel="noopener noreferrer">AIUC</a> are building.</p></article></div></main><footer class="flex flex-col items-center gap-1 py-8"><a href="https://scratch.dev" target="_blank" rel="noopener noreferrer" class="inline-flex items-center text-gray-400 text-sm font-normal no-underline hover:no-underline"><span class="text-sm">Made from</span><img src="/blog/scratch-logo.svg" alt="Scratch" class="h-9 pb-0.5"/></a></footer></div></div>
    <script type="module" src="/blog/posts/fig-application/index-p917g3yn.js"></script>
  </body>
</html>